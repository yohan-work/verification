import os

# ==========================================
# [Phase 2 ì‹¤ìŠµ] ë¡œì»¬ RAG êµ¬í˜„í•˜ê¸°
# ==========================================
#
# "ë¬¸ì„œ ì„ë² ë”© -> ì €ì¥ -> ê²€ìƒ‰" ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
#
# [í•„ìˆ˜ ì„¤ì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬]
# í„°ë¯¸ë„ì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”:
# pip install langchain-community langchain-huggingface chromadb sentence-transformers
#
# ==========================================

try:
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma
    from langchain_core.documents import Document
except ImportError as e:
    print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (Error: {e})")
    print("í„°ë¯¸ë„ì— ë‹¤ìŒì„ ì…ë ¥í•˜ì„¸ìš”:")
    print("pip install langchain-community langchain-huggingface chromadb sentence-transformers")
    exit()

def main():
    print("1. ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
    # í•œêµ­ì–´ ì²˜ë¦¬ì— ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì‚¬ìš© (HuggingFace)
    # CPUì—ì„œë„ ë¬´ë¦¬ ì—†ì´ ëŒì•„ê°€ëŠ” ê²½ëŸ‰ ëª¨ë¸ì…ë‹ˆë‹¤.
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask"
    )
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    # ---------------------------------------------------------
    # 2. ê°€ìƒì˜ ì‚¬ë‚´ ë°ì´í„° ì¤€ë¹„ (Document Loading)
    # company_knowledge.txt íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
    # ---------------------------------------------------------
    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, "company_knowledge.txt")
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            full_text = f.read()
    except FileNotFoundError:
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return

    # ê°„ë‹¨í•˜ê²Œ ì¤„ë°”ê¿ˆ ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤. (ë¹ˆ ì¤„ ì œì™¸)
    # RecursiveCharacterTextSplitter :: v2
    raw_documents = [line.strip() for line in full_text.split('\n') if line.strip()]
    
    # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    docs = [Document(page_content=text) for text in raw_documents]
    print(f"ë¬¸ì„œ {len(docs)}ê°œë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.")

    # ---------------------------------------------------------
    # 3. Vector DB ìƒì„± ë° ì €ì¥ (Indexing)
    # ---------------------------------------------------------
    # ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ì— ë²¡í„° ì €ì¥ì†Œë¥¼ ë§Œë“­ë‹ˆë‹¤.
    print("Vector DBì— ë°ì´í„°ë¥¼ ì €ì¥(Indexing) ì¤‘...")
    vector_store = Chroma.from_documents(
        documents=docs,
        embedding=embedding_model,
        collection_name="company_knowledge"
    )
    print("ì €ì¥ ì™„ë£Œ!")

    # ---------------------------------------------------------
    # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Retrieval)
    # ---------------------------------------------------------
    while True:
        print("\n" + "="*50)
        query = input("ğŸ” ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        
        if query.lower() == 'exit':
            break
            
        print(f"'{query}' ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ (Similarity Search)
        # k=2 : ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 2ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
        results = vector_store.similarity_search(query, k=2)
        
        if not results:
            print("ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            continue
            
        print("\n[ê²€ìƒ‰ ê²°ê³¼]")
        for i, doc in enumerate(results):
            print(f"{i+1}. {doc.page_content}")
            
        print("\n[Tip] ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì— ë„£ìœ¼ë©´ ë‹µë³€ì´ ì™„ì„±ë©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
